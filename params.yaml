TrainingArguments:
  num_train_epochs: 3
  learning_rate: 0.0001
  lr_scheduler_type: reduce_lr_on_plateau
  metric_for_best_model: eval_loss
  greater_is_better: false
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  weight_decay: 0.01
  logging_steps: 10
  evaluation_strategy: steps
  save_steps: 1000000
  gradient_accumulation_steps: 16